{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incomplete-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Building wheels for collected packages: et-xmlfile\n",
      "  Building wheel for et-xmlfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8913 sha256=7c6d4718231cfd6a98a98aa6627e2be017933cdec496a5ee3e1c97da7be70d8e\n",
      "  Stored in directory: /home/saayed/.cache/pip/wheels/bb/9b/ff/8fdf015c95a57224237bf88fbb9e45237cbe8177213bc13732\n",
      "Successfully built et-xmlfile\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.0.1 openpyxl-3.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biblical-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "electrical-border",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reddit API Credentials\n",
    "reddit = praw.Reddit(client_id='z3B2YGC6KNOAhA',\n",
    "                     client_secret='A-iajELfWcsHquxjoOwea3r4BbbBiA',\n",
    "                     password='9S2a8a7hcr!',\n",
    "                     user_agent='CBT by /u/saayed',\n",
    "                     username='saayed')\n",
    "\n",
    "def get_top_subreddit(sr):\n",
    "    x_subreddit = reddit.subreddit(sr).top(limit=1000)\n",
    "    \n",
    "    # Create an empty dictionary to save data\n",
    "    dict = {'body': []}\n",
    "\n",
    "    # Storing the data in the empty dictionary\n",
    "    for post in x_subreddit:\n",
    "        dict['body'].append(post.selftext)\n",
    "                \n",
    "    # Convert the data to pandas dataframe and apply date function\n",
    "    df = pd.DataFrame(dict)\n",
    "   \n",
    "    return df        \n",
    "\n",
    "df = get_top_subreddit('DiaryOfARedditor')\n",
    "\n",
    "# Save it as CSV\n",
    "df.to_csv('rdiary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "norman-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated = pd.read_excel('CBT.xlsx', convert_float=True)\n",
    "df_reddit = pd.read_csv('rdiary.csv')\n",
    "df_reddit = df_reddit.rename(columns={\"body\": \"Automatic Thought\"})\n",
    "df = df_annotated.append(df_reddit, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "breathing-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    text = text.lower() # lowers the corpus\n",
    "    text = re.sub('http\\S+', ' ', str(text)) # removes any url\n",
    "    text = re.sub('n\\'t\\s', ' not ', str(text))\n",
    "    text = re.sub('-(?<!\\d)', ' ', str(text)) # removing hyphens from numbers\n",
    "    text = sp(text) # apply spacy model\n",
    "    text = [w.text for w in text if not w.is_stop] # tokenize and remove stop words\n",
    "    text = sp(' '.join(text)) # join words and apply spacy model again \n",
    "    text = [w.lemma_ for w in text] # lemmatizes the words\n",
    "    stopwords_extra = ['im', 'na', 'u', 'ill', '10184285', '179180', 'as', 'oh', 'av', 'wo', 'nt', 'p', 'm', 'ta', '10000', '6000']\n",
    "    text = [word for word in text if not word in stopwords_extra] # remove additional unnecessary words\n",
    "    text = ' '.join(text)  # join the words back together  \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # removes all punctuation\n",
    "    text = re.sub('[^\\w\\s,]', ' ', str(text)) # removes emoticon and other non characters\n",
    "    text = re.sub('x200b', ' ', str(text)) # removing zero-width space characters\n",
    "    text = re.sub(' cannabi ', ' cannabis ', str(text))\n",
    "    return ' '.join([token for token in text.split()]) # removes trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corporate-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satisfactory-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-discrimination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
